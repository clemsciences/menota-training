{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "about-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin, Doc\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed3d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_menota_annotations_for_spacy():\n",
    "    l = []\n",
    "    with codecs.open(os.path.join(\"data-menota-spacy.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        for book in f:\n",
    "            for line in json.loads(book):\n",
    "                if all([i == \"-\" for i in line[0]]):\n",
    "                    continue\n",
    "                l.append(line)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "capable-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_menota_annotations_for_spacy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eb594ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens 173164\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of tokens {sum([len(i[0]) for i in data])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f626a5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens 16866\n"
     ]
    }
   ],
   "source": [
    "unique_tokens = set()\n",
    "for i in data:\n",
    "    unique_tokens.update(i[0])\n",
    "\n",
    "print(f\"Number of unique tokens {len(unique_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c1d6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tags 1171\n"
     ]
    }
   ],
   "source": [
    "unique_tags = set()\n",
    "for i in data:\n",
    "    unique_tags.update(i[1][\"pos\"])\n",
    "# print(unique_tags)\n",
    "print(f\"Number of unique tags {len(unique_tags)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3fc98ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n",
      "['í', 'vígskǫrðum', 'verja', ',', 'þá', 'er', 'gott', 'at', 'gera', 'hengivígskǫ', 'af', 'léttum', 'viði']\n",
      "['xAP', 'xNC gN nP cD sI', 'xVB fI tPS vA', '-', 'xAV rP', 'xVB fF tPS mIN p3 nS vA', 'xAJ rP gN nS cN sI', 'xIM', 'xVB fI tPS vA', 'xNC gN nP cA sI', 'xAP', 'xAJ rP gM nS cD sI', 'xNC gM nS cD sI']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example\")\n",
    "print(data[0][0])\n",
    "print(data[0][1][\"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conditional-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(training_data):\n",
    "    vocabulary_set = set()\n",
    "    for text, _ in training_data:\n",
    "        for lemma in text:\n",
    "            vocabulary_set.add(lemma)\n",
    "    return vocabulary_set\n",
    "\n",
    "\n",
    "def reduce_tags(pos_tag: str) -> str:\n",
    "    first_part = pos_tag.strip().split(\" \")[0]\n",
    "    if \"|\" in first_part:\n",
    "        return reduce_tags(first_part.split(\"|\")[0])\n",
    "    if first_part.startswith(\"x\"):\n",
    "        return first_part[1:]\n",
    "    elif \"00000\" == first_part:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return pos_tag\n",
    "\n",
    "    \n",
    "def reduce_word(word: str):\n",
    "    #return word.replace(\" \", \"\").lower()\n",
    "    if word == \"-\":\n",
    "        return word\n",
    "    elif word == \",–\":\n",
    "        return \"-\"\n",
    "    elif word == \",–\":\n",
    "        return \",\"\n",
    "    else:\n",
    "        return word.lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\"'\", \"\")\n",
    "\n",
    "\n",
    "def from_text_annotations_to_spacy_training_data(data):\n",
    "    l = []\n",
    "    for sentence, annotation in data:\n",
    "        try:\n",
    "            l.append(([reduce_word(token) for token in sentence],\n",
    "                       dict(pos=[reduce_tags(tag) for tag in annotation[\"pos\"]])))\n",
    "        except AttributeError:\n",
    "            print(sentence)\n",
    "            print(annotation)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polyphonic-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_training_data = from_text_annotations_to_spacy_training_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dc3d696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tags 31, {'AV', 'VP', 'IT', 'AJ', 'NE', 'PD', 'AP', '-', 'RP', 'UA', 'DP', 'NO', 'NC', 'DQ', 'DD', 'EX', 'NA', 'CU', 'PR', 'PQ', 'VB', 'CC', 'PE', 'FN', 'AQ', 'AT', 'NP', 'IM', 'PI', 'FW', 'CS'}\n"
     ]
    }
   ],
   "source": [
    "unique_tags = set()\n",
    "for i in pos_training_data:\n",
    "    unique_tags.update(i[1][\"pos\"])\n",
    "# print(unique_tags)\n",
    "print(f\"Number of unique tags {len(unique_tags)}, {unique_tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "960d008c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n",
      "['í', 'vígskǫrðum', 'verja', ',', 'þá', 'er', 'gott', 'at', 'gera', 'hengivígskǫ', 'af', 'léttum', 'viði']\n",
      "['AP', 'NC', 'VB', '-', 'AV', 'VB', 'AJ', 'IM', 'VB', 'NC', 'AP', 'AJ', 'NC']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example\")\n",
    "print(f\"{pos_training_data[0][0]}\")\n",
    "print(f\"{pos_training_data[0][1]['pos']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0bebd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training(training_data):\n",
    "    vocab = spacy.Vocab(string=create_vocab(training_data))\n",
    "    nlp = spacy.blank(\"is\")\n",
    "    db = DocBin()  # create a DocBin object\n",
    "    text = []\n",
    "    k = 0\n",
    "    for j, item in enumerate(training_data):\n",
    "        words = item[0]\n",
    "        pos = item[1]\n",
    "#         example = Example.from_dict(Doc(vocab, words=words), pos)\n",
    "\n",
    "        try:\n",
    "            #print(j, len(words), len(pos[\"pos\"]), repr(\" \".join(words)))\n",
    "            #print([i for i in zip(words, pos[\"pos\"])])\n",
    "\n",
    "            #example = Example.from_dict(nlp.make_doc(\" \".join(words)), pos)\n",
    "            example = Example.from_dict(nlp.make_doc(\" \".join(words)), dict(words=words,tags=pos[\"pos\"]))\n",
    "\n",
    "            db.add(example.reference)\n",
    "            text.append(\" \".join(words))\n",
    "        except ValueError:\n",
    "            k += 1\n",
    "            #print(len(words), \" \".join(words))\n",
    "            #print(len(pos[\"pos\"]), pos)\n",
    "            #print(\"problème\")\n",
    "            #break\n",
    "    print(f\"{k} problematic cases\")\n",
    "            \n",
    "    return db, \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d01b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_data(training_data):\n",
    "    random.shuffle(training_data)\n",
    "    valid = training_data[:500]\n",
    "    train = training_data[500:]\n",
    "\n",
    "    train, train_corpus = create_training(train)\n",
    "    train.to_disk(\"./data/train.spacy\")\n",
    "    with codecs.open(\"./data/train_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(train_corpus)\n",
    "\n",
    "    valid, valid_corpus = create_training(valid)\n",
    "    valid.to_disk(\"./data/valid.spacy\")\n",
    "    with codecs.open(\"./data/valid_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(valid_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "primary-dining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 problematic cases\n",
      "2 problematic cases\n"
     ]
    }
   ],
   "source": [
    "save_training_data(pos_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "constant-decrease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['hverir',\n",
       "  'eftir',\n",
       "  'skulu',\n",
       "  'renna',\n",
       "  ',',\n",
       "  'er',\n",
       "  'maðr',\n",
       "  'er',\n",
       "  'hǫggvinn',\n",
       "  'á',\n",
       "  'þingi',\n",
       "  'capitulus',\n",
       "  'en',\n",
       "  'ef',\n",
       "  'maðr',\n",
       "  'verðr',\n",
       "  'hǫggvinn',\n",
       "  'á',\n",
       "  '-',\n",
       "  '-'],\n",
       " {'pos': ['DQ',\n",
       "   'AP',\n",
       "   'VB',\n",
       "   'VB',\n",
       "   '-',\n",
       "   'CS',\n",
       "   'NC',\n",
       "   'VB',\n",
       "   'VB',\n",
       "   'AP',\n",
       "   'NC',\n",
       "   'FW',\n",
       "   'CC',\n",
       "   'CS',\n",
       "   'NC',\n",
       "   'VB',\n",
       "   'VB',\n",
       "   'AP',\n",
       "   '-',\n",
       "   '-']})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1266d2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AV', 'VP', 'IT', 'NE', 'PD', 'AP', '-', 'RP', 'UA', 'DP', 'NO', 'NC', 'DQ', 'DD', 'EX', 'CS', 'CU', 'NA', 'PR', 'PQ', 'VB', 'CC', 'PE', 'FN', 'AT', 'AQ', 'NP', 'IM', 'PI', 'FW', 'AJ']\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "for item in pos_training_data:\n",
    "    labels = item[1][\"pos\"]\n",
    "    for label in labels:\n",
    "        tags.append(label)\n",
    "tags = list(set(tags))\n",
    "with open (\"tag.json\", \"w\", encoding='utf-8') as f:\n",
    "    print(tags)\n",
    "    json.dump(tags, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f11b1ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\n",
      "============================ Data file validation ============================\u001B[0m\n",
      "corpora\n",
      "[+] Pipeline can be initialized with data\n",
      "[+] Corpus is loadable\n",
      "\u001B[1m\n",
      "=============================== Training stats ===============================\u001B[0m\n",
      "Language: is\n",
      "Training pipeline: tok2vec, tagger\n",
      "9577 training docs\n",
      "498 evaluation docs\n",
      "[!] 13 training examples also in evaluation data\n",
      "\u001B[1m\n",
      "============================== Vocab & Vectors ==============================\u001B[0m\n",
      "[i] 164000 total word(s) in the data (15372 unique)\n",
      "[i] No word vectors present in the package\n",
      "\u001B[1m\n",
      "=========================== Part-of-speech Tagging ===========================\u001B[0m\n",
      "[i] 31 label(s) in train data\n",
      "\u001B[1m\n",
      "================================== Summary ==================================\u001B[0m\n",
      "[+] 2 checks passed\n",
      "[!] 1 warning\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug data config-v3.2.cfg --paths.train ./data/train.spacy --paths.dev ./data/valid.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b39901e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m\n",
      "============================= Config validation =============================\u001B[0m\n",
      "\u001B[1m\n",
      "===================== Config validation for [initialize] =====================\u001B[0m\n",
      "\u001B[1m\n",
      "====================== Config validation for [training] ======================\u001B[0m\n",
      "corpora\n",
      "[+] Config is valid\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy debug config config-v3.2.cfg --paths.train ./data/train.spacy --paths.dev ./data/valid.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "uniform-corruption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config-v3.2.cfg --paths.train ./data/train.spacy --paths.dev ./data/valid.spacy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}